import boto3
import gzip
import io
import json
import os
import sys
import csv
import re
import pytz
import datetime
from datetime import datetime, timedelta


log_pattern = re.compile(r"\d{4}-\d{2}-\d{2}-\d{2}-\d{2}-\d{2}")



output_dir = './s3/'  # ë¡œì»¬ì— ì €ì¥í•  ì¶œë ¥ ë””ë ‰í† ë¦¬

contact_ids = set()
file_names = set()



def get_contact_timestamp(contact_id,region,instance_id):
    """AWS Connect Contact Flow ì •ë³´ë¥¼ ê°€ì ¸ì™€ JSON íŒŒì¼ë¡œ ì €ì¥"""

    client = boto3.client("connect", region_name=region)

    response = client.describe_contact(
        InstanceId=instance_id,
        ContactId=contact_id
    )

    # init -1ë¶„, disconnect +10ë¶„
    initiation_time = datetime.fromisoformat(str(response["Contact"]["InitiationTimestamp"])).astimezone(pytz.UTC) - timedelta(minutes=1)
    if response["Contact"].get("DisconnectTimestamp"):
        disconnect_time = datetime.fromisoformat(str(response["Contact"]["DisconnectTimestamp"])).astimezone(pytz.UTC) + timedelta(minutes=10)
        return initiation_time.replace(tzinfo=None),disconnect_time.replace(tzinfo=None)
    else:
        return initiation_time.replace(tzinfo=None),None

    

def get_analysis_object(env,contact_id,region,instance_id):
    
    """ëŒ€í™” ë‚´ìš©ì„ ê°€ì ¸ì™€ì„œ íŒŒì¼ë¡œ ì €ì¥"""

    bucket_name = f"aicc-{env}-an2-s3-acn-storage"

    initiation_time,disconnect_time = get_contact_timestamp(contact_id,region,instance_id)

    prefix = "Analysis/Voice/"+"/".join(str(disconnect_time if disconnect_time else initiation_time).split(" ")[0].split("-"))+"/"+contact_id
    
    
    # S3 í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    s3_client = boto3.client('s3', region_name=region)

    response = s3_client.list_objects_v2(Bucket=bucket_name,Prefix=prefix)
    
    for obj in response.get('Contents', []):

        s3_key = obj['Key']

        if contact_id in s3_key:
            print("Transcript Found")
            try:
                data = s3_client.get_object(Bucket=bucket_name, Key=s3_key)
                conversation_data = data['Body'].read().decode('utf-8')

                transcript = json.loads(conversation_data).get('Transcript',[])
                
                return transcript
            except botocore.exceptions.ClientError as e:
                error_code = e.response['Error']['Code']
                print(f"âŒ Failed to get transcript from S3: {error_code}")
                if error_code == "AccessDenied":
                    print("ğŸ”’ Access denied. Likely due to KMS Decrypt permission or cross-region resource.")
                elif error_code == "NoSuchKey":
                    print("ğŸ“‚ S3 key not found.")
                else:
                    print(f"âš ï¸ Unhandled S3 error: {e}")
                return []

            except Exception as e:
                print(f"â— Unexpected error while fetching transcript: {e}")
                return []

    return []

# S3ì—ì„œ Gzip íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ì••ì¶•ì„ í‘¼ í›„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
def decompress_gzip_from_s3(bucket_name, s3_key, region):
    try:
        # S3 í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        s3_client = boto3.client('s3', region_name=region)
        # S3 ê°ì²´ ë‹¤ìš´ë¡œë“œ
        response = s3_client.get_object(Bucket=bucket_name, Key=s3_key)
        gzip_data = response['Body'].read()  # íŒŒì¼ì—ì„œ Gzip ë°”ì´ë„ˆë¦¬ ë°ì´í„°ë¥¼ ì½ì–´ì˜µë‹ˆë‹¤.

        # ë©”ëª¨ë¦¬ì—ì„œ gzip ë°ì´í„°ë¥¼ ì½ì–´ì˜µë‹ˆë‹¤.
        try:
            with gzip.GzipFile(fileobj=io.BytesIO(gzip_data), mode='rb') as f:
                decompressed_data = f.read().decode('utf-8')  # ì••ì¶•ì„ í’€ê³  í…ìŠ¤íŠ¸ë¡œ ë³µì›
        except Exception as e:
            print(f'gzip failed : {e}')
        return decompressed_data
    except botocore.exceptions.ClientError as e:
        error_code = e.response['Error']['Code']
        print(f"âŒ Failed to get transcript from S3: {error_code}")
        if error_code == "AccessDenied":
            print("ğŸ”’ Access denied. Likely due to KMS Decrypt permission or cross-region resource.")
        elif error_code == "NoSuchKey":
            print("ğŸ“‚ S3 key not found.")
        else:
            print(f"âš ï¸ Unhandled S3 error: {e}")
        return []

    except Exception as e:
        print(f"â— Unexpected error while fetching transcript: {e}")
        return []

# S3 ê²½ë¡œì—ì„œ ëª¨ë“  íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
def decompress_datadog_logs(env, contact_id, instance_id,region):
    # print(contact_id)
    bucket_name = f"aicc-{env}-an2-s3-adf-datadog-backup"

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
    # os.makedirs(output_dir, exist_ok=True)
    s3_client = boto3.client('s3', region_name=region)

    logs = []
    datadog_lambda_logs = []

    initiation_time,disconnect_time = get_contact_timestamp(contact_id,region,instance_id)

    prefix_list = set()
    prefix_list.add("/".join(str(disconnect_time).split(" ")[0].split("-")))
    prefix_list.add("/".join(str(initiation_time).split(" ")[0].split("-")))
    # print(contact_id,initiation_time,disconnect_time,prefix)
    for prefix in prefix_list:
        response = s3_client.list_objects_v2(Bucket=bucket_name,Prefix=prefix)

        s3_keys = []
        for obj in response.get('Contents', []):
            s3_key = obj['Key']
            
            # S3 ê°ì²´ê°€ Gzip íŒŒì¼ì¸ ê²½ìš°ì—ë§Œ ì²˜ë¦¬
            try:
                match = log_pattern.search(s3_key)
                if not match:
                    continue
                log_time = datetime.strptime(match.group(), "%Y-%m-%d-%H-%M-%S").replace(tzinfo=None)

                if initiation_time <= log_time <= disconnect_time:
                    s3_keys.append(s3_key)


            except Exception as e:
                print(f"Skipping non-gzip file {s3_key} : {e}")
    lambda_log_groups = set()
    for key in s3_keys:
    # Gzip íŒŒì¼ì„ ë³µì›í•˜ì—¬ ì²˜ë¦¬
        decompressed_text = decompress_gzip_from_s3(bucket_name, key, region)

        decompressed_text = decompressed_text.replace("}{","}\n{")

        # if "serialNumber" not in decompressed_text: # í‚¤ì›Œë“œ ê²€ìƒ‰
        #     continue

        # print(f"Processing file: {key}") 

        # with open(f"{output_dir}{key.split("/")[4]}", "w", encoding="utf-8") as f:
        #     f.write(decompressed_text)

        # f.close()

        data = decompressed_text.splitlines()
        
        for line in data:
            json_data = json.loads(line)

            #### filter logic start ####
            try:
                if contact_id in line and json_data.get("logGroup"):
                    
                    if "/aws/connect/kal-servicecenter" in json_data.get("logGroup"):
                        for event in json_data['logEvents']:

                            message = json.loads(event.get("message"))
                            if message.get("ContactId") == contact_id:
                                logs.append(message)
                            # :
                            #     contact_ids.add(message.get("ContactId"))
                    elif "/aws/lmd" in json_data.get("logGroup"):
                        lambda_log_groups.add(json_data.get("logGroup"))
                        for event in json_data['logEvents']:

                            message = json.loads(event.get("message"))
                            if message.get("ContactId") == contact_id:
                                datadog_lambda_logs.append(message)




            except Exception as e: 
                print(e)

    logs = sorted(logs, key=lambda x : x["Timestamp"], reverse=False) # To-do : Timestamp ìˆœì´ ì•„ë‹ˆë¼ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì •ë ¬í•´ì•¼ í• ë“¯
    datadog_lambda_logs = sorted(datadog_lambda_logs, key=lambda x : x["timestamp"], reverse=False)

    lambda_logs = {}
    for lambda_log_group in lambda_log_groups:
        
        function_name = lambda_log_group.split("/")[4]

        f_logs = []
        for datadog_lambda_log in datadog_lambda_logs:

            if function_name in datadog_lambda_log.get("service"):
                f_logs.append(datadog_lambda_log)

        lambda_logs[function_name] = f_logs

    # JSON íŒŒì¼ ì €ì¥    
    output_json_path = f"./virtual_env/contact_flow_{contact_id}.json"
    lambda_output_json_path = f"./virtual_env/lambda_logs_{contact_id}.json"

    if len(logs) > 0:
        with open(output_json_path, "w", encoding="utf-8") as json_file:
            json.dump(logs, json_file, ensure_ascii=False, indent=4)
            print(f"{output_json_path} saved!!!")

    if len(lambda_logs) > 0:
        with open(lambda_output_json_path, "w", encoding="utf-8") as json_file:
            json.dump(lambda_logs, json_file, ensure_ascii=False, indent=4)
            print(f"{lambda_output_json_path} saved!!!")

    #### filter logic end ####
    return logs, lambda_logs

    
def single_int_to_str(i):
    return "0"+str(i) if len(str(i))==1 else str(i)



# S3 ê²½ë¡œì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° ì²˜ë¦¬
# decompress_datadog_logs(bucket_name, contact_id)


